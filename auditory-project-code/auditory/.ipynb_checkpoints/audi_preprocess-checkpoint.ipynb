{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import hub\n",
    "from script import vggish_input, vggish_params\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "model.postprocess = False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sort(x : str):\n",
    "    if '.' in x:\n",
    "        # 将文件名字用_进行分割\n",
    "        x = x.rpartition('_')\n",
    "        # 将x用.进行分割，最后拿到数字\n",
    "        x = x[0][2:4]\n",
    "    else:\n",
    "        x = 0\n",
    "    return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor([]).to(device)\n",
    "ss_path = '/scratch/bs4283/auditory_data/data/re_train_voice/training_data'\n",
    "fileList = os.listdir('/scratch/bs4283/auditory_data/data/re_train_voice/training_data')\n",
    "fileList.sort(key=list_sort)\n",
    "for filename in fileList:\n",
    "    filepath = os.path.join(ss_path,filename)\n",
    "    with torch.no_grad():  \n",
    "        re = model(filepath).to(device)\n",
    "    train_data = torch.cat((train_data,re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_data,'/scratch/bs4283/auditory_data/data/re_train_voice/training_data_new.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.ones([60,200,128])\n",
    "for kk in range(60):\n",
    "    training_data[kk,:,:] = train_data[kk*200:(kk+1)*200,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training data to get binary weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        self.linear = nn.Linear(128,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(data1,data2):\n",
    "    data1 = data1.numpy()\n",
    "    data2 = data2.numpy()\n",
    "    sub_diff = np.setdiff1d(data1,data2)\n",
    "    r2 = torch.randperm(11800)[0:200]\n",
    "    ss = data2[r2]\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = 1 # 1 - 60\n",
    "train_x1 = train_data[speakers*200:(speakers+1)*200,:]\n",
    "\n",
    "sub = torch.arange(12000)\n",
    "sub_cat = torch.arange(speakers*200,(speakers+1)*200,1)\n",
    "sub_diff = make_train_data(sub_cat,sub)\n",
    "\n",
    "train_x2 = train_data[sub_diff,:]\n",
    "train_x = torch.cat((train_x1, train_x2), 0)\n",
    "y1 = torch.ones(200)\n",
    "y2 = torch.zeros(200)\n",
    "train_y = torch.cat((y1,y2),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_bin = torch.tensor([]).to(device)\n",
    "bias_bin = torch.tensor([]).to(device)\n",
    "for hh in range(60):\n",
    "    train_x1 = train_data[hh*200:(hh+1)*200,:]\n",
    "    sub = torch.arange(12000)\n",
    "    sub_cat = torch.arange(hh*200,(hh+1)*200,1)\n",
    "    sub_diff = make_train_data(sub_cat,sub)        \n",
    "    train_x2 = train_data[sub_diff,:]\n",
    "    train_x = torch.cat((train_x1, train_x2), 0)\n",
    "    y1 = torch.ones(200)\n",
    "    y2 = torch.zeros(200)\n",
    "    train_y = torch.cat((y1,y2),0).to(device)\n",
    "    \n",
    "    model_binary = LogisticRegression().to(device)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    lr = 0.01 \n",
    "    optimizer = torch.optim.SGD(model_binary.parameters(), lr=lr, momentum=0.9) \n",
    "    \n",
    "    for iteration in range(1000):        \n",
    "        y_pred = model_binary(train_x.to(device)).to(device)             \n",
    "        loss = loss_fn(y_pred.squeeze(), train_y)   \n",
    "        loss.requires_grad_(True)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()              \n",
    "        optimizer.step()   \n",
    "    \n",
    "    ww = model_binary.linear.weight.to(device)\n",
    "    bi = model_binary.linear.bias.to(device)\n",
    "    weight_bin = torch.cat((weight_bin.to(device),ww.to(device)))\n",
    "    bias_bin = torch.cat([bias_bin.to(device),bi.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_bin[59,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_bin[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = LogisticRegression().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LL(nn.Module):\n",
    "    def __init__(self,W,B):\n",
    "        super(LL,self).__init__()\n",
    "        self.weight = W\n",
    "        self.bias= B\n",
    "        self.linear = nn.Linear(128,1)\n",
    "        self.linear.weight = nn.Parameter(self.weight)\n",
    "        self.linear.bias = nn.Parameter(self.bias)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LL = LL(W=torch.unsqueeze(weight_bin[45,:],0),B=bias_bin[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1 = train_data[45*200:(45+1)*200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=model_LL(train_x1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sort_tunning(x : str):\n",
    "    if '.' in x:\n",
    "        # 将文件名字用_进行分割\n",
    "        x = x.rpartition('.')\n",
    "        # 将x用.进行分割，最后拿到数字\n",
    "        x = x[0][-1]\n",
    "    else:\n",
    "        x = 0\n",
    "    return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_amats(speaker,layer,sevec):\n",
    "    attnmats=[]\n",
    "    fileList = os.listdir('/scratch/bs4283/auditory_data/data/tunning_value')\n",
    "    ss_path = '/scratch/bs4283/auditory_data/data/tunning_value'\n",
    "    fileList.sort(key=list_sort_tunning)\n",
    "    filename = fileList[layer]\n",
    "    filepath = os.path.join(ss_path,filename)\n",
    "    value = torch.load(filepath)\n",
    "    \n",
    "    for ii in range(6):\n",
    "        if ii == layer : \n",
    "            k=value[speaker,:]/(torch.max(torch.abs(value),0).values)\n",
    "            k=torch.unsqueeze(torch.unsqueeze(k,1),1)\n",
    "            k[k == torch.inf] = 0 \n",
    "            k[k == -torch.inf] = 0 \n",
    "            if ii == 0:\n",
    "                amat = torch.ones([64, 96, 64]).to(device) + torch.tile(k,[1,96,64]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            elif ii == 1:\n",
    "                amat = torch.ones([128, 48, 32]).to(device)  + torch.tile(k,[1,48,32]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            elif ii == 2:\n",
    "                amat = torch.ones([256, 24, 16]).to(device)  + torch.tile(k,[1,24,16]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            elif ii == 3:\n",
    "                amat = torch.ones([256, 24, 16]).to(device)  + torch.tile(k,[1,24,16]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            elif ii == 4:\n",
    "                amat = torch.ones([512, 12, 8]).to(device)  + torch.tile(k,[1,12,8]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            elif ii == 5:\n",
    "                amat = torch.ones([512, 12, 8]).to(device)  + torch.tile(k,[1,12,8]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            if ii == 0:\n",
    "                amat = torch.ones([64, 96, 64]).to(device) \n",
    "                attnmats.append(amat)\n",
    "            elif ii == 1:\n",
    "                amat = torch.ones([128, 48, 32]).to(device) \n",
    "                attnmats.append(amat)\n",
    "            elif ii == 2:\n",
    "                amat = torch.ones([256, 24, 16]).to(device) \n",
    "                attnmats.append(amat)\n",
    "            elif ii == 3:\n",
    "                amat = torch.ones([256, 24, 16]).to(device) \n",
    "                attnmats.append(amat)\n",
    "            elif ii == 4:\n",
    "                amat = torch.ones([512, 12, 8]).to(device) \n",
    "                attnmats.append(amat)\n",
    "            elif ii == 5:\n",
    "                amat = torch.ones([512, 12, 8]).to(device) \n",
    "                attnmats.append(amat)\n",
    "    return attnmats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16ish(nn.Module):\n",
    "    def __init__(self,binary_weight,binary_bias,tunning):\n",
    "        super(VGG16ish, self).__init__()\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        self.tunning = tunning\n",
    "        self.binary_weight = binary_weight.to(device)\n",
    "        self.binary_bias = binary_bias.to(device)\n",
    "        base_model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "        model_list = list(base_model.children())\n",
    "        self.model_layer0 = model_list[0][0:2]\n",
    "        self.model_layer1 = model_list[0][2:5]\n",
    "        self.model_layer2 = model_list[0][5:8]\n",
    "        self.model_layer3 = model_list[0][8:10]\n",
    "        self.model_layer4 = model_list[0][10:13]\n",
    "        self.model_layer5 = model_list[0][13:15]\n",
    "        self.model_layer6 = model_list[0][15] \n",
    "        self.linear1 = model_list[1]\n",
    "        self.linear2 = nn.Linear(128,1)\n",
    "        self.linear2.weight = nn.Parameter(self.binary_weight)\n",
    "        self.linear2.bias = nn.Parameter(self.binary_bias)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model_layer0(x)\n",
    "        out = torch.mul(out,self.tunning[0])\n",
    "        out = self.model_layer1(out)\n",
    "        out = torch.mul(out,self.tunning[1])\n",
    "        out = self.model_layer2(out)\n",
    "        out = torch.mul(out,self.tunning[2])\n",
    "        out = self.model_layer3(out)\n",
    "        out = torch.mul(out,self.tunning[3])\n",
    "        out = self.model_layer4(out)\n",
    "        out = torch.mul(out,self.tunning[4])\n",
    "        out = self.model_layer5(out)\n",
    "        out = torch.mul(out,self.tunning[5])\n",
    "        out = self.model_layer6(out)\n",
    "        out = torch.transpose(out, 1, 3)\n",
    "        out = torch.transpose(out, 1, 2)\n",
    "        out = out.contiguous().view(75, -1)\n",
    "        \n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = make_amats(45,0,0)\n",
    "model_vggll = VGG16ish(binary_weight= torch.unsqueeze(weight_bin[45,:],0) ,binary_bias = bias_bin[45],tunning = t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = torch.tensor([])\n",
    "for filename in fileList[45*200:(45*200+75)]:\n",
    "    filepath = os.path.join(ss_path,filename)\n",
    "    re = vggish_input.wavfile_to_examples(filepath)\n",
    "    pre_data = torch.cat((pre_data,re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vggll(pre_data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features[0].weight == model_vggll.model_layer0[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vggll.model_layer0[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(weight_bin,'/scratch/bs4283/auditory_data/data/re_train_voice/binary_weight.pt')\n",
    "torch.save(bias_bin,'/scratch/bs4283/auditory_data/data/re_train_voice/binary_bias.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        self.linear = nn.Linear(128,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(data,num):\n",
    "    sub = np.arange(0,60)\n",
    "    data_new = torch.tensor([])\n",
    "    for ll in range(60):\n",
    "        if ll ==num:\n",
    "            continue\n",
    "        data_new = torch.cat((data_new,data[ll,:,:]))   \n",
    "    ran = torch.randint(0, 11799, (200,))\n",
    "    ss=data_new[ran,:]\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data1 = torch.load('/scratch/bs4283/auditory_data/data/re_train_voice/training_data1.pt')\n",
    "train_reshape = torch.zeros([20,60,128,5])\n",
    "for jj in range(5):\n",
    "    for ii in range(60):\n",
    "        ss = training_data1[jj*1200:(jj+1)*1200,:]\n",
    "        train_reshape[:,ii,:,jj]=ss[ii*20:(ii+1)*20,:]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data2 = torch.load('/scratch/bs4283/auditory_data/data/re_train_voice/training_data2.pt')\n",
    "train_reshape2 = torch.zeros([20,60,128,5])\n",
    "for jj in range(5):\n",
    "    for ii in range(60):\n",
    "        ss2 = training_data2[jj*1200:(jj+1)*1200,:]\n",
    "        train_reshape2[:,ii,:,jj]=ss2[ii*20:(ii+1)*20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res1 = torch.zeros(100,60,128)\n",
    "train_res2 = torch.zeros(100,60,128)\n",
    "for kk in range(5):\n",
    "    train_res1[kk*20:(kk+1)*20,:,:] = train_reshape[:,:,:,kk]\n",
    "    train_res2[kk*20:(kk+1)*20,:,:] = train_reshape2[:,:,:,kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resh = torch.hstack((train_res1.permute(1,0,2),train_res2.permute(1,0,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_resh,'/scratch/bs4283/auditory_data/data/re_train_voice/training_reshape.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        self.linear = nn.Linear(128,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('/scratch/bs4283/auditory_data/data/re_train_voice/training_reshape.pt')\n",
    "# data structrue [60,200,128] , for speakers * numbers * dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(data,num):\n",
    "    sub = np.arange(0,60)\n",
    "    data_new = torch.tensor([])\n",
    "    for ll in range(60):\n",
    "        if ll ==num:\n",
    "            continue\n",
    "        data_new = torch.cat((data_new,data[ll,:,:]))   \n",
    "    ran = torch.randint(0, 11799, (200,))\n",
    "    ss=data_new[ran,:]\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = 1 # 1 - 60\n",
    "train_x1 = data[speakers,:,:]\n",
    "train_x2 = make_train_data(data,speakers)\n",
    "train_x = torch.cat((train_x1, train_x2), 0)\n",
    "y1 = torch.ones(200)\n",
    "y2 = torch.zeros(200)\n",
    "train_y = torch.cat((y1,y2),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_binary = LogisticRegression()\n",
    "loss_fn = nn.BCELoss()\n",
    "lr = 0.01 \n",
    "optimizer = torch.optim.SGD(model_binary.parameters(), lr=lr, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1000):        \n",
    "        y_pred = model_binary(train_x.data)             \n",
    "        loss = loss_fn(y_pred.squeeze(), train_y)   \n",
    "        loss.requires_grad_(True)\n",
    "        loss.backward()              \n",
    "        optimizer.step()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb=model_binary.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwe = torch.vstack((ww,ww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe = torch.cat([bbe,bb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight file for 60 speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_bin = torch.tensor([])\n",
    "bias_bin = torch.tensor([])\n",
    "for hh in range(60):\n",
    "    train_x1 = data[hh,:,:]\n",
    "    train_x2 = make_train_data(data,hh)\n",
    "    train_x = torch.cat((train_x1, train_x2), 0)\n",
    "    y1 = torch.ones(200)\n",
    "    y2 = torch.zeros(200)\n",
    "    train_y = torch.cat((y1,y2),0)\n",
    "    \n",
    "    model_binary = LogisticRegression()\n",
    "    loss_fn = nn.BCELoss()\n",
    "    lr = 0.01 \n",
    "    optimizer = torch.optim.SGD(model_binary.parameters(), lr=lr, momentum=0.9) \n",
    "    \n",
    "    for iteration in range(1000):        \n",
    "        y_pred = model_binary(train_x.data)             \n",
    "        loss = loss_fn(y_pred.squeeze(), train_y)   \n",
    "        loss.requires_grad_(True)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()              \n",
    "        optimizer.step()   \n",
    "    \n",
    "    ww = model_binary.linear.weight\n",
    "    bi = model_binary.linear.bias\n",
    "    weight_bin = torch.cat((weight_bin,ww))\n",
    "    bias_bin = torch.cat([bias_bin,bi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(weight_bin,'/scratch/bs4283/auditory_data/data/re_train_voice/binary_weight.pt')\n",
    "torch.save(bias_bin,'/scratch/bs4283/auditory_data/data/re_train_voice/binary_bias.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tunning value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "model.postprocess = False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_layer = list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tunning1 = model_base_layer[0][0:2]\n",
    "model_tunning2 = model_base_layer[0][2:5]\n",
    "model_tunning3 = model_base_layer[0][5:8]\n",
    "model_tunning4 = model_base_layer[0][8:10]\n",
    "model_tunning5 = model_base_layer[0][10:13]\n",
    "model_tunning6 = model_base_layer[0][13:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script import vggish_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sort(x : str):\n",
    "    if '.' in x:\n",
    "        # 将文件名字用_进行分割\n",
    "        x = x.rpartition('_')\n",
    "        # 将x用.进行分割，最后拿到数字\n",
    "        x = x[0][2:4]\n",
    "    else:\n",
    "        x = 0\n",
    "    return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_path = '/scratch/bs4283/auditory_data/data/re_tunning_voice'\n",
    "fileList = os.listdir('/scratch/bs4283/auditory_data/data/re_tunning_voice')\n",
    "fileList.sort(key=list_sort)\n",
    "value_1 = torch.tensor([]).to(device)\n",
    "input_v = torch.tensor([]).to(device)\n",
    "for filename in fileList:\n",
    "    filepath = os.path.join(ss_path,filename)\n",
    "    vid = vggish_input.wavfile_to_examples(filepath).to(device)\n",
    "    input_v = torch.cat([input_v,vid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():    \n",
    "    value1=model_tunning1(input_v).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning = torch.tensor([]).to(device)\n",
    "for kk in range(60):\n",
    "    value = torch.mean(value1[kk*150:(kk+1)*150,:,:,:],[2,3])\n",
    "    c = torch.mean(value,0)\n",
    "    tunning = torch.cat([tunning,torch.unsqueeze(c,0)])\n",
    "torch.save(tunning,'/scratch/bs4283/auditory_data/data/tunning_value/tunning_value1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():    \n",
    "    value2 = model_tunning2(value1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning = torch.tensor([]).to(device)\n",
    "for kk in range(60):\n",
    "    value = torch.mean(value2[kk*150:(kk+1)*150,:,:,:],[2,3])\n",
    "    c = torch.mean(value,0)\n",
    "    tunning = torch.cat([tunning,torch.unsqueeze(c,0)])\n",
    "torch.save(tunning,'/scratch/bs4283/auditory_data/data/tunning_value/tunning_value2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():    \n",
    "    value3 = model_tunning3(value2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning3 = torch.tensor([]).to(device)\n",
    "for kk in range(60):\n",
    "    value = torch.mean(value3[kk*150:(kk+1)*150,:,:,:],[2,3])\n",
    "    c = torch.mean(value,0)\n",
    "    tunning3 = torch.cat([tunning3,torch.unsqueeze(c,0)])\n",
    "torch.save(tunning3,'/scratch/bs4283/auditory_data/data/tunning_value/tunning_value3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():    \n",
    "    value4 = model_tunning4(value3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning4 = torch.tensor([]).to(device)\n",
    "for kk in range(60):\n",
    "    value = torch.mean(value4[kk*150:(kk+1)*150,:,:,:],[2,3])\n",
    "    c = torch.mean(value,0)\n",
    "    tunning4 = torch.cat([tunning4,torch.unsqueeze(c,0)])\n",
    "torch.save(tunning4,'/scratch/bs4283/auditory_data/data/tunning_value/tunning_value4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():    \n",
    "    value5 = model_tunning5(value4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning5 = torch.tensor([]).to(device)\n",
    "for kk in range(60):\n",
    "    value = torch.mean(value5[kk*150:(kk+1)*150,:,:,:],[2,3])\n",
    "    c = torch.mean(value,0)\n",
    "    tunning5 = torch.cat([tunning5,torch.unsqueeze(c,0)])\n",
    "torch.save(tunning5,'/scratch/bs4283/auditory_data/data/tunning_value/tunning_value5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():    \n",
    "    value6 = model_tunning6(value5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning6 = torch.tensor([]).to(device)\n",
    "for kk in range(60):\n",
    "    value = torch.mean(value6[kk*150:(kk+1)*150,:,:,:],[2,3])\n",
    "    c = torch.mean(value,0)\n",
    "    tunning6 = torch.cat([tunning6,torch.unsqueeze(c,0)])\n",
    "torch.save(tunning6,'/scratch/bs4283/auditory_data/data/tunning_value/tunning_value6.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=model_base_layer[0][14:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():    \n",
    "    o = r(value6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_path = '/scratch/bs4283/auditory_data/data/re_tunning_voice'\n",
    "fileList = os.listdir('/scratch/bs4283/auditory_data/data/re_tunning_voice')\n",
    "fileList.sort(key=list_sort)\n",
    "value_2 = torch.tensor([]).to(device)\n",
    "for filename in fileList:\n",
    "    filepath = os.path.join(ss_path,filename)\n",
    "    vid = vggish_input.wavfile_to_examples(filepath).to(device)\n",
    "    l1=model_tunning1(vid).to(device)\n",
    "    l2 = model_tunning2(l1).to(device)\n",
    "    value_2 = torch.cat([value_2,l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = vggish_input.wavfile_to_examples('/scratch/bs4283/auditory_data/data/re_tunning_voice/1_16_38.wav').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "l1=model_tunning1(vid).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cc=torch.mean(l1,[2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = torch.tensor([]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = model_base_layer[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = torch.cat([kk,l1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re(l1).to(device).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_path = '/scratch/bs4283/auditory_data/data/re_tunning_voice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model('/scratch/bs4283/auditory_data/data/re_tunning_voice/1_16_38.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_path = '/scratch/bs4283/auditory_data/data/re_tunning_voice'\n",
    "fileList = os.listdir('/scratch/bs4283/auditory_data/data/re_tunning_voice')\n",
    "fileList.sort(key=list_sort)\n",
    "for filename in os.listdir('/scratch/bs4283/auditory_data/data/re_tunning_voice'):\n",
    "    filepath = os.path.join(ss_path,filename)\n",
    "    re = model(filepath).to(device)\n",
    "    train_data = torch.hstack((train_data,re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = os.listdir('/scratch/bs4283/auditory_data/data/re_tunning_voice')\n",
    "fileList.sort(key=list_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tunning2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd='1_16_38.wav'.rpartition('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd[0][2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sort(x : str):\n",
    "    if '.' in x:\n",
    "        # 将文件名字用_进行分割\n",
    "        x = x.rpartition('_')\n",
    "        # 将x用.进行分割，最后拿到数字\n",
    "        x = x[0][2:4]\n",
    "    else:\n",
    "        x = 0\n",
    "    return int(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=os.listdir('/scratch/bs4283/auditory_data/data/re_tunning_voice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.sort(key=list_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((ww,ww)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = 1\n",
    "data_new = torch.tensor([])\n",
    "for ll in range(60):\n",
    "    if ll ==speakers:\n",
    "        continue\n",
    "    data_new = torch.cat((data_new,data[ll,:,:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran = torch.randint(0, 11799, (200,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=data_new[ran,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_binary = LogisticRegression()\n",
    "loss_fn = nn.BCELoss()\n",
    "lr = 0.01 #学习率 \n",
    "optimizer = torch.optim.SGD(model_binary.parameters(), lr=lr, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1000):        \n",
    "        y_pred = model_binary(train_x)             \n",
    "        loss = loss_fn(y_pred.squeeze(), train_y)              \n",
    "        loss.backward()              \n",
    "        optimizer.step()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer=os.listdir('/scratch/bs4283/auditory_data/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data1 = torch.load('/scratch/bs4283/auditory_data/data/re_train_voice/training_data1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = [];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jj in range(5):\n",
    "    for ii in range(60):\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data1[20:40,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = training_data1[0:1200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = ss.reshape([20,60,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss[20:40,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss[:,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reshape = torch.zeros([20,60,128])\n",
    "for ii in range(60):\n",
    "    train_reshape[:,ii,:]=ss[ii*20:(ii+1)*20,:]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reshape[:,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
