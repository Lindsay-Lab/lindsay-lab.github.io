{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import hub\n",
    "from script import vggish_input, vggish_params\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_weight = '/scratch/bs4283/auditory_data/data_20sample/binary_weight3.pt'\n",
    "binary_bias = '/scratch/bs4283/auditory_data/data_20sample/binary_bias3.pt'\n",
    "tunning_value_folder = '/scratch/bs4283/auditory_data/data_20sample/tunnning_value/new_tunning_value/normalize/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sort_tunning(x : str):\n",
    "    if '.' in x:\n",
    "        # 将文件名字用_进行分割\n",
    "        x = x.rpartition('.')\n",
    "        # 将x用.进行分割，最后拿到数字\n",
    "        x = x[0][-1]\n",
    "    else:\n",
    "        x = 0\n",
    "    return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attnmats=[]\n",
    "fileList = os.listdir('/scratch/bs4283/auditory_data/data_20sample/tunnning_value/new_tunning_value/normalize')\n",
    "ss_path = '/scratch/bs4283/auditory_data/data_20sample/tunnning_value/new_tunning_value/normalize'\n",
    "fileList.sort(key=list_sort_tunning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = fileList[0]\n",
    "filepath = os.path.join(ss_path,filename)\n",
    "value = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_qul = []\n",
    "for layer in range(6):\n",
    "    filename = fileList[layer]\n",
    "    filepath = os.path.join(ss_path,filename)\n",
    "    value = torch.load(filepath)\n",
    "    value = value.cpu().detach().numpy()\n",
    "    max_v = np.amax(np.abs(value),axis=0)\n",
    "    v_qul.append(max_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_qul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x1463e8325610>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e83258b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e83428e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e8342b80>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82d1b80>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82d1e20>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82dfe20>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82eb100>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82f7100>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82f73a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e83043a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e8304640>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x1463e8325b50>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e8325df0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e8342e20>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82d1100>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82df100>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82df3a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82eb3a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82eb640>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82f7640>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82f78e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e83048e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e8304b80>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x1463e8311250>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e8342640>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82d18e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82dfb80>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82ebe20>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e8304100>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x1463e83420d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82d13a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82df640>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82eb8e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82f7b80>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e8304e20>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x1463e8342370>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82d1640>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82df8e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82ebb80>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e82f7e20>,\n",
       "  <matplotlib.lines.Line2D at 0x1463e8299100>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(v_qul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_amats(speaker,layer,sevec):\n",
    "    attnmats=[]\n",
    "    fileList = os.listdir('/scratch/bs4283/auditory_data/data_20sample/tunnning_value')\n",
    "    ss_path = '/scratch/bs4283/auditory_data/data_20sample/tunnning_value'\n",
    "    fileList.sort(key=list_sort_tunning)\n",
    "    filename = fileList[layer]\n",
    "    filepath = os.path.join(ss_path,filename)\n",
    "    value = torch.load(filepath)\n",
    "    \n",
    "    for ii in range(6):\n",
    "        if ii == layer : \n",
    "            k=value[speaker,:]/(torch.max(torch.abs(value),0).values)\n",
    "            k=torch.unsqueeze(torch.unsqueeze(k,1),1)\n",
    "            k[k == torch.inf] = 0 \n",
    "            k[k == -torch.inf] = 0 \n",
    "            k = torch.from_numpy(np.nan_to_num(k.cpu().numpy())).to(device)\n",
    "            \n",
    "            if ii == 0:\n",
    "                amat = torch.ones([64, 96, 64]).to(device) + torch.tile(k,[1,96,64]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            elif ii == 1:\n",
    "                amat = torch.ones([128, 48, 32]).to(device)  + torch.tile(k,[1,48,32]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            elif ii == 2:\n",
    "                amat = torch.ones([256, 24, 16]).to(device)  + torch.tile(k,[1,24,16]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            elif ii == 3:\n",
    "                amat = torch.ones([256, 24, 16]).to(device)  + torch.tile(k,[1,24,16]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            elif ii == 4:\n",
    "                amat = torch.ones([512, 12, 8]).to(device)  + torch.tile(k,[1,12,8]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            elif ii == 5:\n",
    "                amat = torch.ones([512, 12, 8]).to(device)  + torch.tile(k,[1,12,8]).to(device) *sevec\n",
    "                attnmats.append(amat)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            if ii == 0:\n",
    "                amat = torch.ones([64, 96, 64]).to(device) \n",
    "                attnmats.append(amat)\n",
    "            elif ii == 1:\n",
    "                amat = torch.ones([128, 48, 32]).to(device) \n",
    "                attnmats.append(amat)\n",
    "            elif ii == 2:\n",
    "                amat = torch.ones([256, 24, 16]).to(device) \n",
    "                attnmats.append(amat)\n",
    "            elif ii == 3:\n",
    "                amat = torch.ones([256, 24, 16]).to(device) \n",
    "                attnmats.append(amat)\n",
    "            elif ii == 4:\n",
    "                amat = torch.ones([512, 12, 8]).to(device) \n",
    "                attnmats.append(amat)\n",
    "            elif ii == 5:\n",
    "                amat = torch.ones([512, 12, 8]).to(device) \n",
    "                attnmats.append(amat)\n",
    "    return attnmats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16ish(nn.Module):\n",
    "    def __init__(self,binary_weight,binary_bias,tunning):\n",
    "        super(VGG16ish, self).__init__()\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        self.tunning = tunning\n",
    "        self.binary_weight = binary_weight.to(device)\n",
    "        self.binary_bias = binary_bias.to(device)\n",
    "        base_model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "        model_list = list(base_model.children())\n",
    "        self.model_layer0 = model_list[0][0:2]\n",
    "        self.model_layer1 = model_list[0][2:5]\n",
    "        self.model_layer2 = model_list[0][5:8]\n",
    "        self.model_layer3 = model_list[0][8:10]\n",
    "        self.model_layer4 = model_list[0][10:13]\n",
    "        self.model_layer5 = model_list[0][13:15]\n",
    "        self.model_layer6 = model_list[0][15] \n",
    "        self.linear1 = model_list[1]\n",
    "        self.linear2 = nn.Linear(128,1)\n",
    "        self.linear2.weight = nn.Parameter(self.binary_weight)\n",
    "        self.linear2.bias = nn.Parameter(self.binary_bias)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model_layer0(x)\n",
    "        out = torch.mul(out,self.tunning[0])\n",
    "        out = self.model_layer1(out)\n",
    "        out = torch.mul(out,self.tunning[1])\n",
    "        out = self.model_layer2(out)\n",
    "        out = torch.mul(out,self.tunning[2])\n",
    "        out = self.model_layer3(out)\n",
    "        out = torch.mul(out,self.tunning[3])\n",
    "        out = self.model_layer4(out)\n",
    "        out = torch.mul(out,self.tunning[4])\n",
    "        out = self.model_layer5(out)\n",
    "        out = torch.mul(out,self.tunning[5])\n",
    "        out = self.model_layer6(out)\n",
    "        out = torch.transpose(out, 1, 3)\n",
    "        out = torch.transpose(out, 1, 2)\n",
    "        out = out.contiguous().view(x.size(0), -1)\n",
    "        \n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script import vggish_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sort_data(x : str):\n",
    "    if '.' in x:\n",
    "        # 将文件名字用_进行分割\n",
    "        x = x.rpartition('.')\n",
    "        # 将x用.进行分割，最后拿到数字\n",
    "        if x[0][1] == '-':      \n",
    "            x = x[0][0]\n",
    "        else:\n",
    "            x=x[0][0:2]\n",
    "    else:\n",
    "        x = 0\n",
    "    return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat = 2\n",
    "#layer = 5 \n",
    "astrgs=np.arange(0,1.,.5)\n",
    "data_num = 75\n",
    "data_folder = '/scratch/bs4283/auditory_data/data_20sample/test_voice_20'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 0\n",
    "fileList_data = os.listdir('/scratch/bs4283/auditory_data/data_20sample/combine_test_20')\n",
    "fileList_data.sort(key=list_sort_data)\n",
    "sub = torch.arange(9000)\n",
    "sub = sub.numpy()\n",
    "sub_cat = torch.arange(cat*150,(cat+1)*150,1)\n",
    "sub_cat = sub_cat.numpy()\n",
    "sub_diff = np.setdiff1d(sub,sub_cat)\n",
    "r1 = torch.randperm(150)\n",
    "r2 = torch.randperm(8850)\n",
    "data_poi = np.array(fileList_data)[sub_cat[r1.numpy()]]\n",
    "data_neg = np.array(fileList_data)[sub_diff[r2.numpy()]]\n",
    "poi_data = torch.tensor([]).to(device)\n",
    "neg_data = torch.tensor([]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk in range(data_num):\n",
    "    poi_path = os.path.join(data_folder,data_poi[kk])\n",
    "    p = vggish_input.wavfile_to_examples(poi_path).to(device)\n",
    "    poi_data = torch.cat([poi_data,p])\n",
    "\n",
    "\n",
    "orde = -1\n",
    "while neg_data.size(dim=0) < 75:\n",
    "    orde = orde + 1 \n",
    "    filename = data_neg[orde].rpartition('.')\n",
    "    if int(filename[0][3:5]) == cat+1:\n",
    "           continue\n",
    "    neg_path = os.path.join(data_folder,data_neg[orde])\n",
    "    n = vggish_input.wavfile_to_examples(neg_path).to(device)\n",
    "    neg_data = torch.cat([neg_data,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.load(binary_weight)\n",
    "bin_b = torch.load(binary_bias)[cat]\n",
    "bin_w = w[cat,:]\n",
    "bin_w = torch.unsqueeze(bin_w,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = np.ones([2,6])\n",
    "nr = np.ones([2,6])\n",
    "for jj in range(6):\n",
    "    at = -1\n",
    "    for ast in astrgs:\n",
    "        at = at+1\n",
    "        t = make_amats(cat,jj,ast)\n",
    "        model_v = VGG16ish(binary_weight = bin_w,binary_bias = bin_b ,tunning = t)            \n",
    "        model_v.eval()\n",
    "        p_result = model_v(poi_data)\n",
    "        n_result  = model_v(neg_data)\n",
    "        p_rate = torch.sum(p_result>0.5).cpu().numpy()/75\n",
    "        n_rate = torch.sum(n_result<0.5).cpu().numpy()/75\n",
    "        pr[at,jj] = p_rate\n",
    "        nr[at,jj] = n_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sub_list[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = torch.tensor([1,2,3,4,5,6,8,10,11,12,13,28,36,43,47,52,56,57,58,59])-1\n",
    "cat_s = -1 \n",
    "for cat in sub_list:\n",
    "    cat_s = cat_s + 1 \n",
    "    fileList_data = os.listdir('/scratch/bs4283/auditory_data/data_20sample/combine_test_20')\n",
    "    fileList_data.sort(key=list_sort_data)\n",
    "    sub = torch.arange(3000)\n",
    "    sub = sub.numpy()\n",
    "    sub_cat = torch.arange(cat_s*150,(cat_s+1)*150,1)\n",
    "    sub_cat = sub_cat.numpy()\n",
    "    sub_diff = np.setdiff1d(sub,sub_cat)\n",
    "    r1 = torch.randperm(150)\n",
    "    r2 = torch.randperm(2850)\n",
    "    data_poi = np.array(fileList_data)[sub_cat[r1.numpy()]]\n",
    "    data_neg = np.array(fileList_data)[sub_diff[r2.numpy()]]\n",
    "    poi_data = torch.tensor([]).to(device)\n",
    "    neg_data = torch.tensor([]).to(device)\n",
    "    \n",
    "    for kk in range(data_num):\n",
    "        poi_path = os.path.join(data_folder,data_poi[kk])\n",
    "        p = vggish_input.wavfile_to_examples(poi_path).to(device)\n",
    "        poi_data = torch.cat([poi_data,p])\n",
    "\n",
    "\n",
    "    orde = -1\n",
    "    while neg_data.size(dim=0) < 75:\n",
    "        orde = orde + 1 \n",
    "        filename = data_neg[orde].rpartition('.')\n",
    "        if int(filename[0][3:5]) == cat+1:\n",
    "               continue\n",
    "        neg_path = os.path.join(data_folder,data_neg[orde])\n",
    "        n = vggish_input.wavfile_to_examples(neg_path).to(device)\n",
    "        neg_data = torch.cat([neg_data,n])\n",
    "        \n",
    "        \n",
    "    w = torch.load(binary_weight)\n",
    "    bin_b = torch.load(binary_bias)[cat_s]\n",
    "    bin_w = w[cat_s,:]\n",
    "    bin_w = torch.unsqueeze(bin_w,0)\n",
    "    \n",
    "    pr = np.ones([2,6])\n",
    "    nr = np.ones([2,6])\n",
    "    for jj in range(6):\n",
    "        at = -1\n",
    "        for ast in astrgs:\n",
    "            at = at+1\n",
    "            t = make_amats(cat_s,jj,ast)\n",
    "            model_v = VGG16ish(binary_weight = bin_w,binary_bias = bin_b ,tunning = t)            \n",
    "            model_v.eval()\n",
    "            p_result = model_v(poi_data)\n",
    "            n_result  = model_v(neg_data)\n",
    "            p_rate = torch.sum(p_result>0.5).cpu().numpy()/75\n",
    "            n_rate = torch.sum(n_result<0.5).cpu().numpy()/75\n",
    "            pr[at,jj] = p_rate\n",
    "            nr[at,jj] = n_rate\n",
    "    np.save('/scratch/bs4283/auditory_data/data_20sample/result2/cat'+ str(cat.numpy()) + '_poi.npy',pr)\n",
    "    np.save('/scratch/bs4283/auditory_data/data_20sample/result2/cat'+ str(cat.numpy()) + '_neg.npy',nr)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for raw test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sort_data(x : str):\n",
    "    if '.' in x:\n",
    "\n",
    "        x = x.rpartition('.')\n",
    "\n",
    "        if x[0][1] == '-':      \n",
    "            x = x[0][0]\n",
    "        else:\n",
    "            x=x[0][2:4]\n",
    "    else:\n",
    "        x = 0\n",
    "    return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = torch.tensor([1,2,3,4,5,6,8,10,11,12,13,28,36,43,47,52,56,57,58,59])-1\n",
    "cat_s = -1 \n",
    "for cat in sub_list:\n",
    "    cat_s = cat_s + 1 \n",
    "    fileList_data = os.listdir('/scratch/bs4283/auditory_data/data_20sample/test_voice_20')\n",
    "    fileList_data.sort(key=list_sort_data)\n",
    "    sub = torch.arange(3000)\n",
    "    sub = sub.numpy()\n",
    "    sub_cat = torch.arange(cat_s*150,(cat_s+1)*150,1)\n",
    "    sub_cat = sub_cat.numpy()\n",
    "    sub_diff = np.setdiff1d(sub,sub_cat)\n",
    "    r1 = torch.randperm(150)\n",
    "    r2 = torch.randperm(2850)\n",
    "    data_poi = np.array(fileList_data)[sub_cat[r1.numpy()]]\n",
    "    data_neg = np.array(fileList_data)[sub_diff[r2.numpy()]]\n",
    "    poi_data = torch.tensor([]).to(device)\n",
    "    neg_data = torch.tensor([]).to(device)\n",
    "    \n",
    "    for kk in range(data_num):\n",
    "        poi_path = os.path.join(data_folder,data_poi[kk])\n",
    "        p = vggish_input.wavfile_to_examples(poi_path).to(device)\n",
    "        poi_data = torch.cat([poi_data,p])\n",
    "\n",
    "\n",
    "    orde = -1\n",
    "    while neg_data.size(dim=0) < 75:\n",
    "        orde = orde + 1 \n",
    "        filename = data_neg[orde].rpartition('.')\n",
    " #       if int(filename[0][3:5]) == cat+1:\n",
    " #             continue\n",
    "        neg_path = os.path.join(data_folder,data_neg[orde])\n",
    "        n = vggish_input.wavfile_to_examples(neg_path).to(device)\n",
    "        neg_data = torch.cat([neg_data,n])\n",
    "        \n",
    "        \n",
    "    w = torch.load(binary_weight)\n",
    "    bin_b = torch.load(binary_bias)[cat_s]\n",
    "    bin_w = w[cat_s,:]\n",
    "    bin_w = torch.unsqueeze(bin_w,0)\n",
    "    \n",
    "    pr = np.ones([2,6])\n",
    "    nr = np.ones([2,6])\n",
    "    for jj in range(6):\n",
    "        at = -1\n",
    "        for ast in astrgs:\n",
    "            at = at+1\n",
    "            t = make_amats(cat_s,jj,ast)\n",
    "            model_v = VGG16ish(binary_weight = bin_w,binary_bias = bin_b ,tunning = t)            \n",
    "            model_v.eval()\n",
    "            p_result = model_v(poi_data)\n",
    "            n_result  = model_v(neg_data)\n",
    "            p_rate = torch.sum(p_result>0.5).cpu().numpy()/75\n",
    "            n_rate = torch.sum(n_result<0.5).cpu().numpy()/75\n",
    "            pr[at,jj] = p_rate\n",
    "            nr[at,jj] = n_rate\n",
    "    np.save('/scratch/bs4283/auditory_data/data_20sample/result_raw_data/cat'+ str(cat.numpy()) + '_poi.npy',pr)\n",
    "    np.save('/scratch/bs4283/auditory_data/data_20sample/result_raw_data/cat'+ str(cat.numpy()) + '_neg.npy',nr)            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
