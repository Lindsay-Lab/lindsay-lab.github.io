{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from scipy.misc import imread, imresize\n",
    "from sklearn import svm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable setting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imtype=1; #images: 1=merge, 2=array\n",
    "cat=19 #0-19 for which object category to attend to and readout for\n",
    "layer = 0 #0-12 for which convolutional layer to apply attention at (if >12, will apply at all layers at 1/10th strength)\n",
    "appwith = 'TCs' #what values to apply attention according to: 'TCs' or 'GRADs'\n",
    "astrgs=np.arange(0,1.,.5) #attention strengths (betas)\n",
    "TCpath='/scratch/bs4283/Data/Data/object_GradsTCs'  #folder with tuning curve and gradient files \n",
    "weight_path = '/scratch/bs4283/Data/Data' #folder with network and classifier weights\n",
    "impath='/scratch/bs4283/Data/Data/objims/images' #folder where image files are kept\n",
    "save_path = '/scratch/bs4283/Data/Data/savefi' #where to save the recording and performance files\n",
    "Ncats = 20 #change to 5 if only using the categories available in the dryad files\n",
    "rec_activity = True #record and save activity or no\n",
    "binary_weight = '/scratch/bs4283/Data/Data/catbins/catbins'\n",
    "\n",
    "imperim=5\n",
    "traintype=3; #shouldnt be changed\n",
    "bsize=15*imperim; # total number of images in each (true pos and true neg) class\n",
    "impercat=np.floor(bsize/(Ncats-1)*1.); leftov=bsize%(Ncats-1)\n",
    "bd=1; #bidirect or pos only (0)\n",
    "attype=1 #1=mult, 2=add, \n",
    "\n",
    "if attype==1:  \n",
    "    astrgs=astrgs\n",
    "elif attype==2:  \n",
    "    lyrBL=[20,100,150,150,240,240,150,150,80,20,20,10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "descat=cat \n",
    "descatpics=np.load(impath+'/merg5_c'+str(descat)+'.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = tf.compat.v1.train.NewCheckpointReader('/scratch/bs4283/Data/Data/catbins/catbins/catbin_'+str(cat)+'.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set gradient or tuning value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gamats(oind,svec,attnmats=[]):\n",
    "    with open(TCpath+'/CATgradsDetectTrainTCs_im'+str(imtype)+'.txt', \"rb\") as fp:  b = pickle.load(fp,encoding='latin1')\n",
    "    for li in range(2):\n",
    "        fv = b[li]\n",
    "        fmvals=np.squeeze(fv[oind,:])/np.amax(np.abs(fv),axis=0)\n",
    "        aval=np.expand_dims(np.expand_dims(fmvals,axis=0),axis=0)\n",
    "        aval[aval==np.inf]=0\n",
    "        aval[aval==-np.inf]=0; aval=np.nan_to_num(aval)\n",
    "        if bd ==0:\n",
    "            aval[aval<0]=0\n",
    "        if attype == 1 :\n",
    "            amat=np.ones((224,224,64))+np.tile(aval,[224,224,1])*svec[li]; amat[amat<0]=0\n",
    "        elif attype == 2 :\n",
    "            amat=np.tile(aval,[224,224,1])*svec[li]*lyrBL[li]\n",
    "        attnmats.append(amat) \n",
    "    for li in range(2,4):\n",
    "        fv = b[li]\n",
    "        fmvals=np.squeeze(fv[oind,:])/np.amax(np.abs(fv),axis=0)\n",
    "        aval=np.expand_dims(np.expand_dims(fmvals,axis=0),axis=0)\n",
    "        aval[aval==np.inf]=0\n",
    "        aval[aval==-np.inf]=0; aval=np.nan_to_num(aval)\n",
    "        if bd ==0:\n",
    "            aval[aval<0]=0\n",
    "        if attype == 1 :\n",
    "            amat=np.ones((112,112,128))+np.tile(aval,[112,112,1])*svec[li]; amat[amat<0]=0\n",
    "        elif attype == 2 :\n",
    "            amat=np.tile(aval,[112,112,1])*svec[li]*lyrBL[li]\n",
    "        attnmats.append(amat)\n",
    "    for li in range(4,7):\n",
    "        fv = b[li]\n",
    "        fmvals=np.squeeze(fv[oind,:])/np.amax(np.abs(fv),axis=0)\n",
    "        aval=np.expand_dims(np.expand_dims(fmvals,axis=0),axis=0)\n",
    "        aval[aval==np.inf]=0\n",
    "        aval[aval==-np.inf]=0; aval=np.nan_to_num(aval)\n",
    "        if bd ==0:\n",
    "            aval[aval<0]=0\n",
    "        if attype == 1 :\n",
    "            amat=np.ones((56,56,256))+np.tile(aval,[56,56,1])*svec[li]; amat[amat<0]=0\n",
    "        elif attype == 2 :\n",
    "            amat=np.tile(aval,[56,56,1])*svec[li]*lyrBL[li]\n",
    "        attnmats.append(amat) \n",
    "    for li in range(7,10):\n",
    "        fv = b[li]\n",
    "        fmvals=np.squeeze(fv[oind,:])/np.amax(np.abs(fv),axis=0)\n",
    "        aval=np.expand_dims(np.expand_dims(fmvals,axis=0),axis=0)\n",
    "        aval[aval==np.inf]=0\n",
    "        aval[aval==-np.inf]=0; aval=np.nan_to_num(aval)\n",
    "        if bd ==0:\n",
    "            aval[aval<0]=0\n",
    "        if attype == 1 :\n",
    "            amat=np.ones((28,28,512))+np.tile(aval,[28,28,1])*svec[li]; amat[amat<0]=0\n",
    "        elif attype == 2 :\n",
    "            amat=np.tile(aval,[28,28,1])*svec[li]*lyrBL[li]\n",
    "        attnmats.append(amat) \n",
    "    for li in range(10,13):\n",
    "        fv = b[li]\n",
    "        fmvals=np.squeeze(fv[oind,:])/np.amax(np.abs(fv),axis=0)\n",
    "        aval=np.expand_dims(np.expand_dims(fmvals,axis=0),axis=0)\n",
    "        aval[aval==np.inf]=0\n",
    "        aval[aval==-np.inf]=0; aval=np.nan_to_num(aval)\n",
    "        if bd ==0:\n",
    "            aval[aval<0]=0\n",
    "        if attype == 1 :\n",
    "            amat=np.ones((14,14,512))+np.tile(aval,[14,14,1])*svec[li]; amat[amat<0]=0\n",
    "        elif attype == 2 :\n",
    "            amat=np.tile(aval,[14,14,1])*svec[li]*lyrBL[li]\n",
    "        attnmats.append(amat) \n",
    "    return attnmats\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_amats(oind,svec): #tuning-based attention\n",
    "\n",
    "    attnmats=[];\n",
    "    with open(TCpath+'/featvecs20_train35_c.txt', \"rb\") as fp:  b = pickle.load(fp,encoding='latin1')\n",
    "    for li in range(2):\n",
    "        fv=b[li]; fmvals=np.squeeze(fv[oind,:]) #fmvals=np.random.permutation(fmvals)# shuffle vals across feat maps\n",
    "        aval=np.expand_dims(np.expand_dims(fmvals,axis=0),axis=0); #ori, fm\n",
    "        aval[aval==np.inf]=0; aval[aval==-np.inf]=0; aval=np.nan_to_num(aval)\n",
    "        if bd==0:\n",
    "            aval[aval<0]=0\n",
    "        if attype==1:\n",
    "            amat=np.ones((224,224,64))+np.tile(aval,[224,224,1])*svec[li]; amat[amat<0]=0\n",
    "        elif attype==2:\n",
    "            amat=np.tile(aval,[224,224,1])*svec[li]*lyrBL[li]; \n",
    "        attnmats.append(amat)\n",
    "        #print amat\n",
    "    for li in range(2,4):\n",
    "        fv=b[li]; fmvals=np.squeeze(fv[oind,:]) #fmvals=np.random.permutation(fmvals)# shuffle vals across feat maps\n",
    "        aval=np.expand_dims(np.expand_dims(fmvals,axis=0),axis=0); #ori, fm\n",
    "        aval[aval==np.inf]=0; aval[aval==-np.inf]=0; aval=np.nan_to_num(aval)\n",
    "        if bd==0:\n",
    "            aval[aval<0]=0\n",
    "        if attype==1:\n",
    "            amat=np.ones((112,112,128))+np.tile(aval,[112,112,1])*svec[li]; amat[amat<0]=0\n",
    "        elif attype==2:\n",
    "            amat=np.tile(aval,[112,112,1])*svec[li]*lyrBL[li]; \n",
    "        attnmats.append(amat)\n",
    "        \n",
    "    for li in range(4,7):\n",
    "        fv=b[li]; fmvals=np.squeeze(fv[oind,:]) #fmvals=np.random.permutation(fmvals)# shuffle vals across feat maps\n",
    "        aval=np.expand_dims(np.expand_dims(fmvals,axis=0),axis=0); #ori, fm\n",
    "        aval[aval==np.inf]=0; aval[aval==-np.inf]=0; aval=np.nan_to_num(aval)\n",
    "        if bd==0:\n",
    "            aval[aval<0]=0\n",
    "        if attype==1:\n",
    "            amat=np.ones((56,56,256))+np.tile(aval,[56,56,1])*svec[li]; amat[amat<0]=0\n",
    "        elif attype==2:\n",
    "            amat=np.tile(aval,[56,56,1])*svec[li]*lyrBL[li]; \n",
    "        attnmats.append(amat)\n",
    "        \n",
    "    for li in range(7,10):\n",
    "        fv=b[li]; fmvals=np.squeeze(fv[oind,:]) #fmvals=np.random.permutation(fmvals)# shuffle vals across feat maps\n",
    "        aval=np.expand_dims(np.expand_dims(fmvals,axis=0),axis=0); #ori, fm\n",
    "        aval[aval==np.inf]=0; aval[aval==-np.inf]=0; aval=np.nan_to_num(aval)\n",
    "        if bd==0:\n",
    "            aval[aval<0]=0\n",
    "        if attype==1:\n",
    "            amat=np.ones((28,28,512))+np.tile(aval,[28,28,1])*svec[li]; amat[amat<0]=0\n",
    "        elif attype==2:\n",
    "            amat=np.tile(aval,[28,28,1])*svec[li]*lyrBL[li]; \n",
    "        attnmats.append(amat)\n",
    "        \n",
    "    for li in range(10,13):\n",
    "        fv=b[li]; fmvals=np.squeeze(fv[oind,:]) #fmvals=np.random.permutation(fmvals)# shuffle vals across feat maps\n",
    "        aval=np.expand_dims(np.expand_dims(fmvals,axis=0),axis=0); #ori, fm\n",
    "        aval[aval==np.inf]=0; aval[aval==-np.inf]=0; aval=np.nan_to_num(aval)\n",
    "        if bd==0:\n",
    "            aval[aval<0]=0\n",
    "        if attype==1:\n",
    "            amat=np.ones((14,14,512))+np.tile(aval,[14,14,1])*svec[li]; amat[amat<0]=0\n",
    "        elif attype==2:\n",
    "            amat=np.tile(aval,[14,14,1])*svec[li]*lyrBL[li]; \n",
    "        attnmats.append(amat)\n",
    "    #print amat\n",
    "    return attnmats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "\n",
    "    def __init__(self,weights,avalue,binary_weight):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.avalue = avalue\n",
    "        self.binary_weight = binary_weight\n",
    "        #conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3,3),stride=1, padding=1, padding_mode='zeros', bias=True)\n",
    "        #conv1_1.weight = nn.parameter(torch.from_numpy(self.weights['conv1_1_W']).permute(2,3,0,1))\n",
    "        #conv1_1.bias = torch.nn.parameter(torch.from_numpy(weights['conv1_1_b']))\n",
    "        #self.conv1_1 = conv1_1\n",
    "        # 3 * 224 * 224\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, 3,bias=True,padding=1)  # 64 * 222 * 222\n",
    "        self.conv1_1.weight = nn.Parameter(torch.from_numpy(self.weights['conv1_1_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv1_1.weight.require_grads = False\n",
    "        self.conv1_1.bias = nn.Parameter(torch.from_numpy(self.weights['conv1_1_b'].astype(np.double)))\n",
    "        self.conv1_1.bias.require_grads = False\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=(1, 1),bias=True)  # 64 * 222* 222\n",
    "        self.conv1_2.weight = nn.Parameter(torch.from_numpy(self.weights['conv1_2_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv1_2.bias = nn.Parameter(torch.from_numpy(self.weights['conv1_2_b'].astype(np.double)))\n",
    "        self.conv1_2.weight.require_grads = False\n",
    "        self.conv1_2.bias.require_grads = False\n",
    "        self.maxpool1 = nn.MaxPool2d((2, 2))  # pooling 64 * 112 * 112\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3,bias=True,padding=1)  # 128 * 110 * 110\n",
    "        self.conv2_1.weight = nn.Parameter(torch.from_numpy(self.weights['conv2_1_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv2_1.bias = nn.Parameter(torch.from_numpy(self.weights['conv2_1_b'].astype(np.double)))\n",
    "        self.conv2_1.weight.require_grads = False\n",
    "        self.conv2_1.bias.require_grads = False\n",
    "        \n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=(1, 1),bias=True)  # 128 * 110 * 110\n",
    "        self.conv2_2.weight = nn.Parameter(torch.from_numpy(self.weights['conv2_2_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv2_2.bias = nn.Parameter(torch.from_numpy(self.weights['conv2_2_b'].astype(np.double)))\n",
    "        self.conv2_2.weight.require_grads = False\n",
    "        self.conv2_2.bias.require_grads = False\n",
    "        \n",
    "        self.maxpool2 = nn.MaxPool2d((2, 2))  # pooling 128 * 56 * 56\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3,bias=True,padding=1)  # 256 * 54 * 54\n",
    "        self.conv3_1.weight = nn.Parameter(torch.from_numpy(self.weights['conv3_1_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv3_1.bias = nn.Parameter(torch.from_numpy(self.weights['conv3_1_b'].astype(np.double)))\n",
    "        self.conv3_1.weight.require_grads = False\n",
    "        self.conv3_1.bias.require_grads = False       \n",
    "        \n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=(1, 1),bias=True)  # 256 * 54 * 54\n",
    "        self.conv3_2.weight = nn.Parameter(torch.from_numpy(self.weights['conv3_2_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv3_2.bias = nn.Parameter(torch.from_numpy(self.weights['conv3_2_b'].astype(np.double)))\n",
    "        self.conv3_2.weight.require_grads = False\n",
    "        self.conv3_2.bias.require_grads = False          \n",
    "        \n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=(1, 1),bias=True)  # 256 * 54 * 54\n",
    "        self.conv3_3.weight = nn.Parameter(torch.from_numpy(self.weights['conv3_3_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv3_3.bias = nn.Parameter(torch.from_numpy(self.weights['conv3_3_b'].astype(np.double)))\n",
    "        self.conv3_3.weight.require_grads = False\n",
    "        self.conv3_3.bias.require_grads = False         \n",
    "        \n",
    "        self.maxpool3 = nn.MaxPool2d((2, 2))  # pooling 256 * 28 * 28\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3,bias=True,padding=1)  # 512 * 26 * 26\n",
    "        self.conv4_1.weight = nn.Parameter(torch.from_numpy(self.weights['conv4_1_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv4_1.bias = nn.Parameter(torch.from_numpy(self.weights['conv4_1_b'].astype(np.double)))\n",
    "        self.conv4_1.weight.require_grads = False\n",
    "        self.conv4_1.bias.require_grads = False        \n",
    "        \n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=(1, 1),bias=True)  # 512 * 26 * 26\n",
    "        self.conv4_2.weight = nn.Parameter(torch.from_numpy(self.weights['conv4_2_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv4_2.bias = nn.Parameter(torch.from_numpy(self.weights['conv4_2_b'].astype(np.double)))\n",
    "        self.conv4_2.weight.require_grads = False\n",
    "        self.conv4_2.bias.require_grads = False         \n",
    "        \n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=(1, 1),bias=True)  # 512 * 26 * 26\n",
    "        self.conv4_3.weight = nn.Parameter(torch.from_numpy(self.weights['conv4_3_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv4_3.bias = nn.Parameter(torch.from_numpy(self.weights['conv4_3_b'].astype(np.double)))\n",
    "        self.conv4_3.weight.require_grads = False\n",
    "        self.conv4_3.bias.require_grads = False         \n",
    "        \n",
    "        self.maxpool4 = nn.MaxPool2d((2, 2))  # pooling 512 * 14 * 14\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, 3,bias=True,padding=1)  # 512 * 12 * 12\n",
    "        self.conv5_1.weight = nn.Parameter(torch.from_numpy(self.weights['conv5_1_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv5_1.bias = nn.Parameter(torch.from_numpy(self.weights['conv5_1_b'].astype(np.double)))\n",
    "        self.conv5_1.weight.require_grads = False\n",
    "        self.conv5_1.bias.require_grads = False        \n",
    "        \n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=(1, 1),bias=True)  # 512 * 12 * 12\n",
    "        self.conv5_2.weight = nn.Parameter(torch.from_numpy(self.weights['conv5_2_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv5_2.bias = nn.Parameter(torch.from_numpy(self.weights['conv5_2_b'].astype(np.double)))\n",
    "        self.conv5_2.weight.require_grads = False\n",
    "        self.conv5_2.bias.require_grads = False            \n",
    "        \n",
    "        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=(1, 1),bias=True)  # 512 * 12 * 12\n",
    "        self.conv5_3.weight = nn.Parameter(torch.from_numpy(self.weights['conv5_3_W'].astype(np.double)).permute(3,2,0,1))\n",
    "        self.conv5_3.bias = nn.Parameter(torch.from_numpy(self.weights['conv5_3_b'].astype(np.double)))\n",
    "        self.conv5_3.weight.require_grads = False\n",
    "        self.conv5_3.bias.require_grads = False          \n",
    "        \n",
    "        self.maxpool5 = nn.MaxPool2d((2, 2))  # pooling 512 * 7 * 7\n",
    "        \n",
    "\n",
    "        # view\n",
    "\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 4096)\n",
    "        self.fc1.weight = nn.Parameter(torch.from_numpy(self.weights['fc6_W'].astype(np.double)).permute(1,0))\n",
    "        self.fc1.bias = nn.Parameter(torch.from_numpy(self.weights['fc6_b'].astype(np.double)))\n",
    "        self.fc1.weight.require_grads = False\n",
    "        self.fc1.bias.require_grads = False\n",
    "        \n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc2.weight = nn.Parameter(torch.from_numpy(self.weights['fc7_W'].astype(np.double)).permute(1,0))\n",
    "        self.fc2.bias = nn.Parameter(torch.from_numpy(self.weights['fc7_b'].astype(np.double)))\n",
    "        self.fc2.weight.require_grads = False\n",
    "        self.fc2.bias.require_grads = False        \n",
    "        \n",
    "        self.fc3 = nn.Linear(4096, 1)\n",
    "        self.fc3.weight = nn.Parameter(torch.from_numpy(self.binary_weight.get_tensor('fc3').astype(np.double)).permute(1,0))\n",
    "        self.fc3.bias = nn.Parameter(torch.from_numpy(self.binary_weight.get_tensor('fcb3').astype(np.double)))\n",
    "        self.fc3.weight.require_grads = False\n",
    "        self.fc3.bias.require_grads = False  \n",
    " #       self.fc3.weight = nn.Parameter(torch.from_numpy(weights['fc8_W'].astype(np.double)).permute(1,0)[:2,:])\n",
    " #       self.fc3.bias = nn.Parameter(torch.from_numpy(weights['fc8_b'].astype(np.double))[:2])\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size(0)即为batch_size\n",
    "        in_size = x.size(0)\n",
    "\n",
    "        out = self.conv1_1(x)  # 222\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[0]).permute(2,0,1))\n",
    "        out = self.conv1_2(out)  # 222\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[1]).permute(2,0,1))\n",
    "        out = self.maxpool1(out)  # 112\n",
    "\n",
    "        out = self.conv2_1(out)  # 110\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[2]).permute(2,0,1))\n",
    "        out = self.conv2_2(out)  # 110\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[3]).permute(2,0,1))\n",
    "        out = self.maxpool2(out)  # 56\n",
    "\n",
    "        out = self.conv3_1(out)  # 54\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[4]).permute(2,0,1))\n",
    "        out = self.conv3_2(out)  # 54\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[5]).permute(2,0,1))\n",
    "        out = self.conv3_3(out)  # 54\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[6]).permute(2,0,1))\n",
    "        out = self.maxpool3(out)  # 28\n",
    "\n",
    "        out = self.conv4_1(out)  # 26\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[7]).permute(2,0,1))\n",
    "        out = self.conv4_2(out)  # 26\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[8]).permute(2,0,1))\n",
    "        out = self.conv4_3(out)  # 26\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[9]).permute(2,0,1))\n",
    "        out = self.maxpool4(out)  # 14\n",
    "\n",
    "        out = self.conv5_1(out)  # 12\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[10]).permute(2,0,1))\n",
    "        out = self.conv5_2(out)  # 12\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[11]).permute(2,0,1))\n",
    "        out = self.conv5_3(out)  # 12\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(out,torch.from_numpy(self.avalue[12]).permute(2,0,1))\n",
    "        out = self.maxpool5(out)  # 7\n",
    "        \n",
    "        out = torch.transpose(out, 1, 3)\n",
    "        out = torch.transpose(out, 1, 2)\n",
    "        # 展平\n",
    "        out = out.contiguous().view(75, -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    " #       out = torch.round(nn.Sigmoid(out))\n",
    " #       out = F.softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_im(image):\n",
    "    image = torch.from_numpy(image)\n",
    "    mean = torch.tensor([123.68, 116.779, 103.939]).reshape([1,1,1,3])\n",
    "    images = image -mean\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "descat=cat \n",
    "if imtype==1:   \n",
    "    descatpics=np.load(impath+'/merg5_c'+str(descat)+'.npz')['arr_0']\n",
    "elif imtype==2:\n",
    "    descatpics=np.load(impath+'/arr5_c'+str(descat)+'.npz')['arr_0']\n",
    "elif imtype==3:\n",
    "    descatpics=np.load(impath+'/cats20_test15_c.npy')\n",
    "\n",
    "tp_batch=np.zeros((bsize,224,224,3)); tplabs=np.ones((bsize,1))\n",
    "tn_batch=np.zeros((bsize,224,224,3)); tnlabs=np.zeros((bsize,1))\n",
    "TPscore=np.zeros((len(astrgs))); TNscore=np.zeros((len(astrgs))); #FN=np.zeros((len(str_vec),xval)); TN=np.zeros((len(str_vec),xval))\n",
    "\n",
    "if rec_activity:\n",
    "    tp_resps1=np.ones((2,bsize,64,len(astrgs))); tp_resps2=np.ones((2,bsize,128,len(astrgs)));  tp_resps3=np.ones((3,bsize,256,len(astrgs)))\n",
    "    tp_resps4=np.ones((3,bsize,512,len(astrgs))); tp_resps5=np.ones((3,bsize,512,len(astrgs)))\n",
    "    tn_resps1=np.ones((2,bsize,64,len(astrgs))); tn_resps2=np.ones((2,bsize,128,len(astrgs)));  tn_resps3=np.ones((3,bsize,256,len(astrgs)))\n",
    "    tn_resps4=np.ones((3,bsize,512,len(astrgs))); tn_resps5=np.ones((3,bsize,512,len(astrgs)))\n",
    "\n",
    "np.random.seed(10) #so each layer has same images etc\n",
    "if imtype==3:\n",
    "    tp_batch=descatpics[descat]\n",
    "else:\n",
    "    for pii in range(15):\n",
    "        tp_batch[pii*imperim:(pii+1)*imperim,:,:,:]=descatpics[pii,np.random.choice(19*5,imperim),:,:,:]\n",
    "del descatpics\n",
    "\n",
    "othercs=np.arange(Ncats); othercs=othercs[othercs!=descat]; cii=0\n",
    "if imtype==3:\n",
    "    othercs=np.random.choice(othercs,15)\n",
    "else:\n",
    "    imspercats=np.ones((Ncats-1))*impercat; imspercats[np.random.choice(Ncats-1,leftov)]+=1\n",
    "# print Ncats, type(descat), othercs\n",
    "for ci in range(len(othercs)):\n",
    "    if imtype==1:\n",
    "        ocatpicsF=np.load(impath+'/merg5_c'+str(othercs[ci])+'.npz')\n",
    "        ocatpics=ocatpicsF['arr_0']; ocatlabs=ocatpicsF['arr_1']; noncind=np.where(ocatlabs[0,:]!=descat)\n",
    "        del ocatpicsF\n",
    "        tn_batch[cii:cii+int(imspercats[ci]),:,:,:]=ocatpics[np.random.choice(15,int(imspercats[ci])),np.random.choice(noncind[0],int(imspercats[ci])),:,:,:]\n",
    "        del ocatpics\n",
    "        cii+=int(imspercats[ci])\n",
    "    elif imtype==2:\n",
    "        ocatpicsF=np.load(impath+'/arr5_c'+str(othercs[ci])+'.npz')\n",
    "        ocatpics=ocatpicsF['arr_0']; ocatlabs=ocatpicsF['arr_1']; \n",
    "        del ocatpicsF\n",
    "        for pici in range(int(imspercats[ci])):            \n",
    "            pic=np.random.choice(15); binocat=np.sum(np.squeeze(ocatlabs[pic,:,:])==descat,axis=1)\n",
    "            print(cii)\n",
    "            print(binocat)\n",
    "            noncind=np.where(binocat==0) #need to check that all 3 others arent descat\n",
    "            tn_batch[cii+pici:cii+pici+1,:,:,:]=ocatpics[pic,np.random.choice(noncind[0]),:,:,:]\n",
    "            cii+=imspercats[ci]\n",
    "    elif imtype==3:\n",
    "        ocatpicsF=descatpics[othercs[ci]]  \n",
    "        tn_batch[ci,:,:,:]=ocatpicsF[np.random.choice(15),:,:,:]\n",
    "        del ocatpicsF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wieghtpath = '/scratch/bs4283/Data/Data'\n",
    "weights = np.load(wieghtpath+'/vgg16_weights.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_n = preprocess_im(tn_batch)\n",
    "a_p = preprocess_im(tp_batch)\n",
    "a_n = a_n.permute(0,3,1,2)\n",
    "a_p = a_p.permute(0,3,1,2)\n",
    "poi_res = np.zeros([2,12])\n",
    "neg_res = np.zeros([2,12])\n",
    "kk = 0\n",
    "lyr=kk\n",
    "laybins=np.zeros((13))\n",
    "if lyr>12:   \n",
    "    laybins=np.ones((13)); astrgs=astrgs/10.0\n",
    "else:\n",
    "    laybins[lyr]=1\n",
    "ai =  1\n",
    "astrg = 0\n",
    "if appwith == 'TCs':    \n",
    "    attnmats=make_amats(descat,laybins*astrg) #descat = cat, cat is 0-19 for which object category to attend to and readout for\n",
    "    attnmats = attnmats[(int(len(attnmats)/13)-1)*13:len(attnmats)+1]\n",
    "elif appwith=='GRADs':\n",
    "    attnmats=make_gamats(descat,laybins*astrg)       \n",
    "model = VGG16(weights=weights,avalue=attnmats,binary_weight = reader)\n",
    "model.eval()\n",
    "tp_score = model(a_p)\n",
    "tn_score = model(a_n)\n",
    "tp_score = tp_score.detach().numpy()\n",
    "tn_score = tn_score.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prr = np.sum(tp_score>0)/75\n",
    "nrr = np.sum(tn_score<0)/75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_res = np.ones([2,12]) * prr\n",
    "neg_res = np.ones([2,12])\n",
    "for kk in range(12):\n",
    "    lyr=kk\n",
    "    laybins=np.zeros((13))\n",
    "    if lyr>12:   \n",
    "        laybins=np.ones((13)); astrgs=astrgs/10.0\n",
    "    else:\n",
    "        laybins[lyr]=1\n",
    "    ai =  1\n",
    "    astrg = 0.5\n",
    "    if appwith == 'TCs':    \n",
    "        attnmats=make_amats(descat,laybins*astrg) #descat = cat, cat is 0-19 for which object category to attend to and readout for\n",
    "        attnmats = attnmats[(int(len(attnmats)/13)-1)*13:len(attnmats)+1]\n",
    "    elif appwith=='GRADs':\n",
    "        attnmats=make_gamats(descat,laybins*astrg)       \n",
    "    model = VGG16(weights=weights,avalue=attnmats,binary_weight = reader)\n",
    "    model.eval()\n",
    "    tp_score = model(a_p)\n",
    "    tn_score = model(a_n)\n",
    "    tp_score = tp_score.detach().numpy()\n",
    "    tn_score = tn_score.detach().numpy()\n",
    "    poi_res[ai,kk] = np.sum(tp_score>0)/75\n",
    "    neg_res[ai,kk] = np.sum(tn_score<0)/75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_n = preprocess_im(tn_batch)\n",
    "a_p = preprocess_im(tp_batch)\n",
    "a_n = a_n.permute(0,3,1,2)\n",
    "a_p = a_p.permute(0,3,1,2)\n",
    "poi_res = np.zeros([2,12])\n",
    "neg_res = np.zeros([2,12])\n",
    "for kk in range(12):\n",
    "    ai = -1 \n",
    "    lyr=kk\n",
    "    laybins=np.zeros((13))\n",
    "    if lyr>12:   \n",
    "        laybins=np.ones((13)); astrgs=astrgs/10.0\n",
    "    else:\n",
    "        laybins[lyr]=1\n",
    "    for astrg in astrgs:\n",
    "        ai =ai+1 \n",
    "        if appwith == 'TCs':\n",
    "            attnmats=make_amats(descat,laybins*astrg) #descat = cat, cat is 0-19 for which object category to attend to and readout for\n",
    "            attnmats = attnmats[(int(len(attnmats)/13)-1)*13:len(attnmats)+1]\n",
    "        elif appwith=='GRADs':\n",
    "            attnmats=make_gamats(descat,laybins*astrg)       \n",
    "        model = VGG16(weights=weights,avalue=attnmats,binary_weight = reader)\n",
    "        model.eval()\n",
    "        tp_score = model(a_p)\n",
    "        tn_score = model(a_n)\n",
    "        tp_score = tp_score.detach().numpy()\n",
    "        tn_score = tn_score.detach().numpy()\n",
    "        poi_res[ai,kk] = np.sum(tp_score>0)/75\n",
    "        neg_res[ai,kk] = np.sum(tn_score<0)/75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai =ai+1 \n",
    "if appwith == 'TCs':    \n",
    "    attnmats=make_amats(descat,laybins*astrg) #descat = cat, cat is 0-19 for which object category to attend to and readout for\n",
    "    attnmats = attnmats[(int(len(attnmats)/13)-1)*13:len(attnmats)+1]\n",
    "elif appwith=='GRADs':\n",
    "    attnmats=make_gamats(descat,laybins*astrg)       \n",
    "model = VGG16(weights=weights,avalue=attnmats,binary_weight = reader)\n",
    "model.eval()\n",
    "tp_score = model(a_p)\n",
    "tn_score = model(a_n)\n",
    "tp_score = tp_score.detach().numpy()\n",
    "tn_score = tn_score.detach().numpy()\n",
    "poi_res[ai,kk] = np.sum(tp_score>0)/75\n",
    "neg_res[ai,kk] = np.sum(tn_score<0)/75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/scratch/bs4283/Data/result/cat19_poi.npy',poi_res)\n",
    "np.save('/scratch/bs4283/Data/result/cat19_neg.npy',neg_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "be719aec424ea912a69a73067bf176ec284b005d7ae438557be19fb5cc392795"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
